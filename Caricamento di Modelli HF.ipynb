{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUq5poQkPL8TuIdCekRalu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Caricamento/Download del modello"],"metadata":{"id":"B0eHxW25vLO0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6xWzcMWtftC"},"outputs":[],"source":["# Per configurare la quantizzazione del modello, richiede `bitsandbytes`\n","# Esempio per il caricamento in 4bit\n","bnb_config = transformers.BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=bfloat16\n",")\n","\n","model_path = \"\" # Repository di Huggingface del modello se si vuole scaricarlo oppure path locale se è già stato salvato\n","\n","# Può succedere che modello e tokenizer siano da scaricare separatamente, in genere sono insieme\n","\n","tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n","\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    #trust_remote_code=True,    # alcuni modelli lo richiedono\n","    quantization_config=bnb_config,\n","    device_map='auto',  # oppure per specificare la GPU n°0:  device_map = {\"\":0}\n","    torch_dtype=torch.float16,\n",")"]},{"cell_type":"markdown","source":["## Pipeline per specificare le proprietà di funzionamento del modello"],"metadata":{"id":"IY_mV8nEvORY"}},{"cell_type":"code","source":["from transformers import pipeline\n","from langchain.llms import HuggingFacePipeline\n","\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512,     # per controllare la lunghezza dell'output\n","    temperature=0.0,\n","    top_p=0.95,\n","    repetition_penalty=1.15,    # default = 1\n",")\n","\n","local_llm = HuggingFacePipeline(pipeline=pipe)"],"metadata":{"id":"6Lu6kFduvJb2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"A0OhaHikvbGC"}},{"cell_type":"code","source":["response = local_llm(\"Tell me the name of the capital of England\")\n","print(response)"],"metadata":{"id":"RcAs2q-fv5at"},"execution_count":null,"outputs":[]}]}
