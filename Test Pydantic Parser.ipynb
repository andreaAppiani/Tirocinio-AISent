{"cells":[{"cell_type":"markdown","id":"9b380f81-8ad4-42c4-969b-965d40f1c3c6","metadata":{"id":"9b380f81-8ad4-42c4-969b-965d40f1c3c6"},"source":["## MODEL SETUP"]},{"cell_type":"code","execution_count":null,"id":"d5f5ef1b-4b65-43bd-b16e-5436dc581434","metadata":{"id":"d5f5ef1b-4b65-43bd-b16e-5436dc581434"},"outputs":[],"source":["%%capture\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","tok_path = \"./models/Llama2-13B-nous-hermes\"\n","model_path = \"./models/Llama2-13B-nous-hermes\"\n","tokenizer = AutoTokenizer.from_pretrained(tok_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path,\n","                                             load_in_8bit = True,\n","                                             device_map=\"auto\",\n","                                             torch_dtype=torch.float16,\n","                                            )\n","from transformers import pipeline\n","from langchain.llms import HuggingFacePipeline\n","import torch\n","\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512,\n","    temperature=0.0,\n","    top_p=0.95,\n","    repetition_penalty=1.15,\n",")\n","\n","local_llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"markdown","id":"458867f1-8787-414e-b166-0e78de6a8050","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"458867f1-8787-414e-b166-0e78de6a8050"},"source":["## FALCON SETUP"]},{"cell_type":"code","execution_count":null,"id":"1d2d0f26-98f6-4bd1-a432-c62d5dbdf8ec","metadata":{"id":"1d2d0f26-98f6-4bd1-a432-c62d5dbdf8ec"},"outputs":[],"source":["%%capture\n","from langchain import HuggingFacePipeline\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch\n","\n","model_path = \"./models/Falcon-7B-instruct\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","pipeline = pipeline(\n","    \"text-generation\",\n","    model=model_path,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    device_map = {\"\":0},\n","    max_length=2048,       # Lunghezza massima della risposta\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    trust_remote_code=True,\n",")\n","# Pesa circa 14GB"]},{"cell_type":"code","execution_count":null,"id":"f27826f0-206e-418f-b601-943cfa5bfd19","metadata":{"id":"f27826f0-206e-418f-b601-943cfa5bfd19","outputId":"5698adf2-3b13-4160-d193-f0de4624ecf9"},"outputs":[{"data":{"text/plain":["RWForCausalLM(\n","  (transformer): RWModel(\n","    (word_embeddings): Embedding(65024, 4544)\n","    (h): ModuleList(\n","      (0-31): 32 x DecoderLayer(\n","        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n","        (self_attention): Attention(\n","          (maybe_rotary): RotaryEmbedding()\n","          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n","          (dense): Linear(in_features=4544, out_features=4544, bias=False)\n","          (attention_dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (mlp): MLP(\n","          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n","          (act): GELU(approximate='none')\n","          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",")"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["pipeline.model.eval()"]},{"cell_type":"code","execution_count":null,"id":"cb3dfcf5-7e46-4533-b7c6-a8cb534dd44a","metadata":{"id":"cb3dfcf5-7e46-4533-b7c6-a8cb534dd44a"},"outputs":[],"source":["local_llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"]},{"cell_type":"markdown","id":"bc784e1f-797a-4e24-9ad4-ef41177723f2","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"bc784e1f-797a-4e24-9ad4-ef41177723f2"},"source":["## MPT SETUP"]},{"cell_type":"code","execution_count":null,"id":"41e7670d-a313-4689-8d1d-4dcede0f0301","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"referenced_widgets":["b87ea40cd9234b5f9e02650b1612061e"]},"id":"41e7670d-a313-4689-8d1d-4dcede0f0301","outputId":"1a2d514a-54b4-4618-f65a-b9f97d8f8040"},"outputs":[{"name":"stdout","output_type":"stream","text":["You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n","\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please run\n","\n","python -m bitsandbytes\n","\n"," and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n","CUDA SETUP: Detected CUDA version 117\n","CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n","  warn(msg)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n","  warn(msg)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Europe/Rome')}\n","  warn(msg)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n","  warn(msg)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n","Either way, this might cause trouble in the future:\n","If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n","  warn(msg)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b87ea40cd9234b5f9e02650b1612061e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import transformers\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","path_tok = \"./models/MPT-7B-Instruct/models--EleutherAI--gpt-neox-20b/snapshots/4e49eadb5d14bd22f314ec3f45b69a87b88c7691/\"\n","path_mod = \"./models/MPT-7B-Instruct/models--mosaicml--mpt-7b-instruct/snapshots/bbe7a55d70215e16c00c1825805b81e4badb57d7/\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(path_tok)\n","\n","model = AutoModelForCausalLM.from_pretrained(path_mod,\n","                                            load_in_8bit=True,\n","                                            device_map={\"\":0},\n","                                            trust_remote_code=True,\n","                                            torch_dtype=torch.float16,\n","                                            max_seq_len = 2048\n","                                            )\n","\n","# Occupa circa 14GB"]},{"cell_type":"code","execution_count":null,"id":"a11a920d-511c-4ffb-b236-481f4299552b","metadata":{"id":"a11a920d-511c-4ffb-b236-481f4299552b"},"outputs":[],"source":["import torch\n","from transformers import StoppingCriteria, StoppingCriteriaList\n","\n","model.eval() # probabilmente non necessario ma giusto per sicurezza\n","model.tie_weights()\n","\n","# mtp-7b is trained to add \"<|endoftext|>\" at the end of generations\n","stop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n","\n","# define custom stopping criteria object\n","class StopOnTokens(StoppingCriteria):\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        for stop_id in stop_token_ids:\n","            if input_ids[0][-1] == stop_id:\n","                return True\n","        return False\n","\n","stopping_criteria = StoppingCriteriaList([StopOnTokens()])"]},{"cell_type":"code","execution_count":null,"id":"8ee77770-2a12-491b-b468-e023ab5e98f7","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"8ee77770-2a12-491b-b468-e023ab5e98f7","outputId":"31b9423f-1ac8-4a0b-bd5e-6e9ebc216670"},"outputs":[{"name":"stderr","output_type":"stream","text":["The model 'MPTForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["%%capture\n","from transformers import pipeline\n","from langchain.llms import HuggingFacePipeline\n","import torch\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=2048,\n","    temperature=0,\n","    top_p=0.95,\n","    #top_p = 0.15,\n","    #top_k = 0,\n","    repetition_penalty=1.15,\n","    stopping_criteria=stopping_criteria\n",")\n","\n","local_llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"markdown","id":"01799b2e-03f6-443f-ba8b-78bbf3a79577","metadata":{"id":"01799b2e-03f6-443f-ba8b-78bbf3a79577"},"source":["# PYDANTIC OUTPUT PARSER"]},{"cell_type":"markdown","id":"56d05508-3975-478e-859c-ad156efb5ee2","metadata":{"id":"56d05508-3975-478e-859c-ad156efb5ee2"},"source":["## TEST GPU QUESTION"]},{"cell_type":"code","execution_count":null,"id":"cc6b2ccf-c925-4333-8e29-173d8c62ab1c","metadata":{"id":"cc6b2ccf-c925-4333-8e29-173d8c62ab1c"},"outputs":[],"source":["from langchain.output_parsers import PydanticOutputParser\n","from pydantic import BaseModel, Field, validator\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate"]},{"cell_type":"code","execution_count":null,"id":"bfed700f-6702-4e2d-a8b8-c0996a621ca1","metadata":{"id":"bfed700f-6702-4e2d-a8b8-c0996a621ca1"},"outputs":[],"source":["gpu_text = \"\"\"\n","RTX 4090 is the ultimate GeForce GPU. It's a huge leap forward in performance, efficiency,\n","and AI-powered graphics, it supports the third generation of Ray-Tracing. Experience ultra-high-performance\n","gaming, incredibly detailed virtual worlds,\n","unprecedented productivity, and new ways to create. Includes 24GB of GDDR6X memory to deliver the\n","ultimate experience for gamers and creators, 16384 CUDA cores. Starting at €1,789.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"74d44cf8-742b-49ef-be35-63ba8229e7c6","metadata":{"id":"74d44cf8-742b-49ef-be35-63ba8229e7c6"},"outputs":[],"source":["# Define your desired data structure.\n","class GPU(BaseModel):\n","    name: str = Field(description=\"name of the GPU (graphics card) model\")\n","    memory: int = Field(description=\"number of Gigabytes of memory of the gpu\",\n","                       enum=[6,10,12,24])\n","    price: float = Field(description=\"price of the gpu, answer with the just a float number\")\n","    rtx_support: str = Field(description=\"Wether the gpu supports rtx (ray-tracing) or not\",\n","                             enum=[\"True\", \"False\"])\n","\n","    def get_values():\n","        return ['name','memory','price','rtx_support']\n","\n","    # You can add custom validation logic easily with Pydantic.\n","    @validator('rtx_support')\n","    def check_rtx(cls, field):\n","        if field != \"True\" and field != \"False\":\n","            raise ValueError(\"Badly formed answer!\")\n","        return field"]},{"cell_type":"code","execution_count":null,"id":"135e59a6-8069-4937-a75d-f9b14e446161","metadata":{"id":"135e59a6-8069-4937-a75d-f9b14e446161"},"outputs":[],"source":["parser = PydanticOutputParser(pydantic_object=GPU)\n","\n","gpu_template = \"\"\"\n","The following is a text about a GPU:\n","\n","text: {text}\n","\n","From the text extract the following information: {info}\n","\n","{format_instructions}\n","\n","formatted answer:\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    template= gpu_template,\n","    input_variables=[\"text\",\"info\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","input = prompt.format_prompt(text=gpu_text, info=gpu_info)"]},{"cell_type":"code","execution_count":null,"id":"475879f1-cc28-490a-b790-705ffba3710e","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"475879f1-cc28-490a-b790-705ffba3710e","outputId":"3731c812-dad2-48e1-b01e-1615b7ee2725"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","For the following text about a GPU, extract the following information:\n","\n","['name', 'memory', 'price', 'rtx support']\n","\n","text: \n","RTX 4090 is the ultimate GeForce GPU. It's a huge leap forward in performance, efficiency, \n","and AI-powered graphics, it supports the third generation of Ray-Tracing. Experience ultra-high-performance \n","gaming, incredibly detailed virtual worlds, \n","unprecedented productivity, and new ways to create. Includes 24GB of GDDR6X memory to deliver the \n","ultimate experience for gamers and creators, 16384 CUDA cores. Starting at €1,789.\n","\n","\n","The output should be formatted as a JSON instance that conforms to the JSON schema below.\n","\n","As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n","the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n","\n","Here is the output schema:\n","```\n","{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of the GPU (graphics card) model\", \"type\": \"string\"}, \"memory\": {\"title\": \"Memory\", \"description\": \"number of Gigabytes of memory of the gpu\", \"type\": \"integer\"}, \"price\": {\"title\": \"Price\", \"description\": \"price of the gpu, answer with the just a float number\", \"type\": \"number\"}, \"rtx_support\": {\"title\": \"Rtx Support\", \"description\": \"Wether the gpu supports rtx (ray-tracing) or not, answer with True or False\", \"type\": \"boolean\"}}, \"required\": [\"name\", \"memory\", \"price\", \"rtx_support\"]}\n","```\n","\n","formatted answer:\n","\n"]}],"source":["print(input.to_string())"]},{"cell_type":"code","execution_count":null,"id":"4e94d18a-5ddd-4d7b-a231-344cd2111b87","metadata":{"id":"4e94d18a-5ddd-4d7b-a231-344cd2111b87","outputId":"e89c9235-ff45-40c4-c5e6-83e5f5228256"},"outputs":[{"data":{"text/plain":["466"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["len(tokenizer.tokenize(input.to_string()))"]},{"cell_type":"code","execution_count":null,"id":"76f59e12-8486-4f65-ad8e-486628a2ae44","metadata":{"id":"76f59e12-8486-4f65-ad8e-486628a2ae44","outputId":"19453ef6-930f-452d-ca75-0716502b0e87"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["```\n","{\n","    \"name\": \"RTX 4090\",\n","    \"memory\": 24,\n","    \"price\": 1789.0,\n","    \"rtx_support\": \"True\"\n","}\n","```\n"]}],"source":["output = local_llm(input.to_string())\n","print(output)"]},{"cell_type":"code","execution_count":null,"id":"3ad7c72b-2272-4f3b-8a74-4eb283d32758","metadata":{"id":"3ad7c72b-2272-4f3b-8a74-4eb283d32758","outputId":"d22eacc4-4832-4ac2-c1dc-2d021190a18d"},"outputs":[{"data":{"text/plain":["GPU(name='RTX 4090', memory=24, price=1789.0, rtx_support='True')"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["JSON_output = parser.parse(output)\n","JSON_output"]},{"cell_type":"markdown","id":"f1b5b4c3-4916-4cf7-a7e2-0c5f12afa40f","metadata":{"id":"f1b5b4c3-4916-4cf7-a7e2-0c5f12afa40f"},"source":["## Genera una battuta con setup e formatta il tutto in un json"]},{"cell_type":"code","execution_count":null,"id":"303b3074-4556-4b75-a5b5-9f34a28eb666","metadata":{"id":"303b3074-4556-4b75-a5b5-9f34a28eb666"},"outputs":[],"source":["class Joke(BaseModel):\n","    setup: str = Field(description=\"question to set up a joke, must end with a question mark\")\n","    punchline: str = Field(description=\"answer to resolve the joke\")\n","\n","    # You can add custom validation logic\n","   # @validator('setup')\n","    #def question_ends_with_question_mark(cls, field):\n","     #   if field[-1] != '?':\n","      #      raise ValueError(\"Badly formed question!\")\n","       # return field\n","\n","joke_query = \"Tell me a joke.\"\n","\n","parser = PydanticOutputParser(pydantic_object=Joke)\n","\n","prompt = PromptTemplate(\n","    template=\"Answer the user query: {query}.\\n{format_instructions}\\n formatted answer:\",\n","    input_variables=[\"query\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","\n","_input = prompt.format_prompt(query=joke_query)\n"]},{"cell_type":"code","execution_count":null,"id":"66dd65f7-112c-4ae1-9762-96f21646795f","metadata":{"id":"66dd65f7-112c-4ae1-9762-96f21646795f","outputId":"84d4d517-67da-45d8-af7f-1558d6a6ad19"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \n","{\n","    \"setup\": \"Why don't scientists trust atoms? \",\n","    \"punchline\": \"Because they make up everything!\"\n","}\n"]}],"source":["output = local_llm(_input.to_string())\n","print(output)"]},{"cell_type":"code","execution_count":null,"id":"5d741d78-ea8f-4c42-93cf-dc2fe5565aaa","metadata":{"id":"5d741d78-ea8f-4c42-93cf-dc2fe5565aaa","outputId":"a278ebe1-6615-45ea-d267-a56b63636eb6"},"outputs":[{"data":{"text/plain":["Joke(setup=\"Why don't scientists trust atoms? \", punchline='Because they make up everything!')"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["parser.parse(output)"]},{"cell_type":"markdown","id":"16b848c9-acb2-4b6a-84a0-569c23e3f8d2","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"16b848c9-acb2-4b6a-84a0-569c23e3f8d2"},"source":["## PARSING PER SOLE STRINGHE (obsoleto)"]},{"cell_type":"code","execution_count":null,"id":"e164d3fd-258b-496a-b145-6ff775d7e9c4","metadata":{"id":"e164d3fd-258b-496a-b145-6ff775d7e9c4"},"outputs":[],"source":["from langchain.output_parsers import ResponseSchema\n","from langchain.output_parsers import StructuredOutputParser\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate"]},{"cell_type":"code","execution_count":null,"id":"a296088f-6f8e-4e25-9989-480b50f3829e","metadata":{"id":"a296088f-6f8e-4e25-9989-480b50f3829e"},"outputs":[],"source":["gift_schema = ResponseSchema(name=\"gift\",\n","                             description=\"\"\"Was the item purchased\n","                             as a gift for someone else?\n","                             Answer True if yes,\n","                             False if not or unknown.\"\"\")\n","delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n","                                      description=\"\"\"How many days\n","                                      did it take for the product\n","                                      to arrive? If this\n","                                      information is not found,\n","                                      output -1.\"\"\")\n","price_value_schema = ResponseSchema(name=\"price_value\",\n","                                    description=\"\"\"Extract any\n","                                    sentences about the value or\n","                                    price, and output them as a\n","                                    comma separated Python list.\"\"\")\n","\n","response_schemas = [gift_schema,\n","                    delivery_days_schema,\n","                    price_value_schema]"]},{"cell_type":"code","execution_count":null,"id":"f9db195b-9262-41e7-8cd4-636b1d640bae","metadata":{"id":"f9db195b-9262-41e7-8cd4-636b1d640bae","outputId":"723df504-eacb-48fe-c260-125b2965a8a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n","\n","```json\n","{\n","\t\"gift\": string  // Was the item purchased\n","                             as a gift for someone else? \n","                             Answer True if yes,\n","                             False if not or unknown.\n","\t\"delivery_days\": string  // How many days\n","                                      did it take for the product\n","                                      to arrive? If this \n","                                      information is not found,\n","                                      output -1.\n","\t\"price_value\": string  // Extract any\n","                                    sentences about the value or \n","                                    price, and output them as a \n","                                    comma separated Python list.\n","}\n","```\n"]}],"source":["output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","format_instructions = output_parser.get_format_instructions()\n","print(format_instructions)"]},{"cell_type":"code","execution_count":null,"id":"915059b2-8fdf-4109-82a6-5f2c4e7ed019","metadata":{"id":"915059b2-8fdf-4109-82a6-5f2c4e7ed019"},"outputs":[],"source":["customer_review = \"\"\"\n","This leaf blower is pretty amazing.  It has four settings:\n","candle blower, gentle breeze, windy city, and tornado.\n","It arrived in two days, just in time for my wife's\n","anniversary present.\n","I think my wife liked it so much she was speechless.\n","So far I've been the only one using it, and I've been\n","using it every other morning to clear the leaves on our lawn.\n","It's slightly more expensive than the other leaf blowers\n","out there, but I think it's worth it for the extra features.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"d999d235-26ba-42db-b3f5-96f19021594f","metadata":{"id":"d999d235-26ba-42db-b3f5-96f19021594f"},"outputs":[],"source":["review_template = \"\"\"\n","For the following text, extract the following information:\n","\n","gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n","\n","delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n","\n","price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n","\n","text: {text}\n","\n","{format_instructions}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template=review_template)\n","\n","input = prompt.format_messages(text=customer_review,\n","                                format_instructions=format_instructions)"]},{"cell_type":"code","execution_count":null,"id":"b9f0c93b-950c-4fae-ba66-2b18fb839223","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"b9f0c93b-950c-4fae-ba66-2b18fb839223","outputId":"90efc368-b75e-4895-8e69-50ca14fc8c2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","For the following text, extract the following information:\n","\n","gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n","\n","delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n","\n","price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n","\n","text: \n","This leaf blower is pretty amazing.  It has four settings:\n","candle blower, gentle breeze, windy city, and tornado. \n","It arrived in two days, just in time for my wife's\n","anniversary present. \n","I think my wife liked it so much she was speechless. \n","So far I've been the only one using it, and I've been \n","using it every other morning to clear the leaves on our lawn. \n","It's slightly more expensive than the other leaf blowers \n","out there, but I think it's worth it for the extra features.\n","\n","\n","The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n","\n","```json\n","{\n","\t\"gift\": string  // Was the item purchased\n","                             as a gift for someone else? \n","                             Answer True if yes,\n","                             False if not or unknown.\n","\t\"delivery_days\": string  // How many days\n","                                      did it take for the product\n","                                      to arrive? If this \n","                                      information is not found,\n","                                      output -1.\n","\t\"price_value\": string  // Extract any\n","                                    sentences about the value or \n","                                    price, and output them as a \n","                                    comma separated Python list.\n","}\n","```\n","\n"]}],"source":["print(input[0].content)"]},{"cell_type":"code","execution_count":null,"id":"17d7aaba-fa18-467c-bd77-a6dca4f78d79","metadata":{"id":"17d7aaba-fa18-467c-bd77-a6dca4f78d79","outputId":"2292f110-f4a5-4610-e705-b8e08f127d3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["output = local_llm(input[0].content)\n","print(output)"]},{"cell_type":"code","execution_count":null,"id":"0a078788-260f-4ae3-a5c3-90c8de72271b","metadata":{"id":"0a078788-260f-4ae3-a5c3-90c8de72271b","outputId":"ca4c5adc-8f51-4515-ad32-fb44f2deda71"},"outputs":[{"data":{"text/plain":["{'gift': 'True',\n"," 'delivery_days': '-1',\n"," 'price_value': [\"It's slightly more expensive than the other leaf blowers out there.\",\n","  \"I think it's worth it for the extra features.\"]}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["output_dict = output_parser.parse(output)\n","output_dict"]},{"cell_type":"code","execution_count":null,"id":"343df89a-5929-4e4c-8b6a-020a0e040a93","metadata":{"id":"343df89a-5929-4e4c-8b6a-020a0e040a93","outputId":"3a15cb24-e79e-4528-fb23-3a0228c22fbe"},"outputs":[{"data":{"text/plain":["'True'"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["output_dict[\"gift\"]"]},{"cell_type":"code","execution_count":null,"id":"00899ae8-90f9-4b50-b64a-1ee6759ba565","metadata":{"id":"00899ae8-90f9-4b50-b64a-1ee6759ba565"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[],"collapsed_sections":["9b380f81-8ad4-42c4-969b-965d40f1c3c6","458867f1-8787-414e-b166-0e78de6a8050","bc784e1f-797a-4e24-9ad4-ef41177723f2","56d05508-3975-478e-859c-ad156efb5ee2","f1b5b4c3-4916-4cf7-a7e2-0c5f12afa40f","16b848c9-acb2-4b6a-84a0-569c23e3f8d2"]}},"nbformat":4,"nbformat_minor":5}